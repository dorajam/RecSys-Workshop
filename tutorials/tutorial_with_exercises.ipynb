{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4aLXKwOz73V"
   },
   "source": [
    "# Movie recommendation using Matrix Factorization\n",
    "## IVADO - Online course on Recommender Systems \n",
    "\n",
    "## Contributors: \n",
    "\n",
    "David Berger (davidberger2785 [at] gmail [dot] com)\n",
    "\n",
    "Laurent Charlin (lcharlin [at] gmail [dot] com)\n",
    "\n",
    "Nissan Pow (nissan [dot] pow [at] gmail [dot] com)\n",
    "\n",
    "Dora Jambor (dorajambor [at] gmail [dot] com)\n",
    "\n",
    "Tutorial adapted from content originally created for [IVADO's Workshop on Recommender Systems](https://ivado.ca/en/trainings/workshops/workshop-recommender-systems/), August 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTVq2aDdz73W"
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This tutorial is an introduction to recommender systems. Specifically, we are going to implement an end-to-end recommender system pipeline based on the matrix factorization algorithm. \n",
    "\n",
    "Since this algorithm is strongly associated with the <a href=\"https://en.wikipedia.org/wiki/Netflix_Prize\">Netflix Prize</a>, we will use the <a href=\"https://grouplens.org/datasets/movielens/\">MovieLens</a> dataset to train our model and conduct our experiments. \n",
    "\n",
    "The task in this dataset is to recommend movies that users will most likely enjoy, based on their historical movie preferences. This task is also an important one, as it is still widely used to benchmark progress in the recommender systems research community. With that, let's get things started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FJKuH-Rz73Z"
   },
   "source": [
    "## 1.2 Installing libraries\n",
    "\n",
    "Before we begin, we must make sure to install the libraries for the tutorial. To do this, we can use Python's package installer `pip`. Now go ahead and execute the following cell by selecting it and clicking `shift`+`Enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "id": "3iqjoZDDz73a",
    "outputId": "8d7a1794-5e3f-4535-aa40-d609a2102085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RecSys-Workshop'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 39 (delta 2), reused 33 (delta 2), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (39/39), done.\n",
      "\u001b[33mhint: The '/Users/Dora/Desktop/dev/deep_dives/RecSys-Workshop/tutorials/RecSys-Workshop/.git/hooks/post-checkout' hook was ignored because it's not set as executable.\u001b[m\n",
      "\u001b[33mhint: You can disable this warning with `git config advice.ignoredHook false`.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!rm -rf RecSys-Workshop\n",
    "!git clone https://github.com/dorajam/RecSys-Workshop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HABufSMz73f"
   },
   "source": [
    "To confirm that the installations have completed successfully, import all the libraries and modules we will use for this tutorial by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rKz4rD0z73g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data vizualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add path to find where to import our custom modules from\n",
    "sys.path += ['RecSys-Workshop/tutorials/']\n",
    "\n",
    "# Pre-built module with some helper functions\n",
    "import utilities as utl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOiy1nZ9z73l"
   },
   "source": [
    "We have also prepared some boilerplate functions that we have grouped together in the `utilities` module. It is not necessary for you to look at these in order to complete the tutorial, but if you are curious, it is definitely a good idea to take a look at them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE.png              High_level_2.png    hidden_2.png        recsys_signals.png\r\n",
      "High_level_1.png    hidden.png          hidden_4.png\r\n"
     ]
    }
   ],
   "source": [
    "ls RecSys-Workshop/assets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YA2A1vqz73m"
   },
   "source": [
    "## 1.1 Goal\n",
    "\n",
    "The purpose of a recommender system is to model users' past behaviors with items such that we can predict what an individual user will most likely enjoy in the future. In short, we would like to create recommendations for items that are personalized to each user's interest.\n",
    "\n",
    "The concept of an item might seem a bit fuzzy at first. But in fact, in the world of recommender systems, we always speak about users and items. Items can be anything a user interacts with, ranging from: products, movies, social media posts, news articles, Instagram photos, restaurants, Pinterest pins and the list goes on. Not so surprisingly, majority of internet companies we interact with on a daily basis have some sort of recommender system in the background.\n",
    "\n",
    "These recommender systems try to predict what will the most relevant item to show us given our user-item interactions. These interactions are often categorized as follows: implicit signals (e.g. likes, views, searches, purchases, installations, music listening behavior etc.) and explicit singals (e.g. ratings, reviews). Both types of signals can be used to help us define how to capture the users' interest, i.e. the user preference. And naturally, some signals suggest stronger proxies for user preferences (e.g. purchases, ratings) than others (e.g. views, clicks, time spent looking at an item)\n",
    "\n",
    "<img src = \"./RecSys-Workshop/assets/recsys_signals.png\" width = \"200\">\n",
    "\n",
    "But how do we go about generation recommendations that are personalized to each user's taste?\n",
    "\n",
    "But how do we capture user's taste? Doesn't user taste change over time?  \n",
    "\n",
    "Is the aim to recommend a specific user with suggestions that reinforce her previous choices? \n",
    "Do we want to recommend complementary, sustitute or independent items from what the user has interacted previously? \n",
    "\n",
    "Will we try instead to present her items to which she has not yet been exposed? \n",
    "\n",
    "Each of these options are correct and needs special considerations in order to model them. \n",
    "Without loss of generality, the diagram below simply models the issue of recommendation systems from the perspective of machine learning.\n",
    "\n",
    "<img src = \"./RecSys-Workshop/assets/High_level_1.png\" width = \"500\">\n",
    "\n",
    "Nevertheless, in the context of this tutorial, that is, either the suggestion of movies on streaming platforms such as Netflix or Amazon Prime, we can reduce the problem to a relatively simple task: recommend movies that the user will like according to her past interests. In order to carry out this task, we will use all the preferences of the users, certain associated sociodemographic variables as well as certain characteristics of the movies. Finally, we can refine the diagram as follows:\n",
    "\n",
    "<img src = \"./RecSys-Workshop/assets/High_level_2.png\" width = \"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-h67-Loz73n"
   },
   "source": [
    "## 1.3 The MoviesLens dataset(s)\n",
    "\n",
    "The data used here consist of more or less 100k movie evaluations by 943 users. Over 1,6k movies are available. In addition to the 100k evaluations, additional information related to users and movies is available.\n",
    "\n",
    "We will use three different datasets to carry out our analyses:\n",
    "\n",
    "<ul>\n",
    "<li> Users : related to users' characteristics,\n",
    "<li> Movies : related to movies' characteristics,\n",
    "<li> Ratings : containing over 100k evaluations.\n",
    "</ul>\n",
    "\n",
    "We used the <a href=\"https://pandas.pydata.org/\">Pandas</a> library in order to download and manipulate the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKlYRrdyz73n"
   },
   "source": [
    "### 1.3.1 Users: Download and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "l_vJZJVnz73o",
    "outputId": "909d8e03-259e-49a3-985f-ae921459c706"
   },
   "outputs": [],
   "source": [
    "# Download\n",
    "ROOT_DIR = 'RS-Workshop/'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/ml-100k/')\n",
    "\n",
    "users = pd.read_csv(os.path.join(DATA_DIR, 'u.user'), \n",
    "                        sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Different variables according to the information provided in the readme file\n",
    "users.columns = ['Index', 'Age', 'Gender', 'Occupation', 'Zip code']\n",
    "\n",
    "# Quick overview\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Hh0wLuEBMAaN",
    "outputId": "e7edd107-c6c3-43db-9386-ccce505494ef"
   },
   "outputs": [],
   "source": [
    "print('Number of users x features:', users.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImaeouHkz78d"
   },
   "source": [
    "Before presenting some descriptive statistics related to the population, we transform the users' data in a <a href=\"https://en.wikipedia.org/wiki/List_(abstract_data_type)\">list</a> in order to be able to handle them more easily. The occupation of each user is a string, so we have recoded the 21 possible occupations in Boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "3y4qvczKz78e",
    "outputId": "83aabcba-b989-459b-fdbb-f72811166ec7"
   },
   "outputs": [],
   "source": [
    "# Number of users\n",
    "nb_users = len(users)\n",
    "\n",
    "# Gender: Convert 'M' and 'F' to 0 and 1\n",
    "gender = np.where(np.matrix(users['Gender']) == 'M', 0, 1)[0]\n",
    "\n",
    "print('Shape of gender features:', gender.shape)\n",
    "\n",
    "# Occupation\n",
    "occupation_name = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u.occupation'), \n",
    "                                            sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Boolean transformation of user's occupation\n",
    "occupation_matrix = np.zeros((nb_users, len(occupation_name)))\n",
    "\n",
    "for k in np.arange(nb_users):\n",
    "    occupation_matrix[k, occupation_name.tolist().index(users['Occupation'][k])] = 1\n",
    "\n",
    "print('Shape of user occupation matrix (num of users x num of occupations):', occupation_matrix.shape)\n",
    "\n",
    "# Concatenation of the sociodemographic variables \n",
    "user_attributes = np.concatenate((np.matrix(users['Age']), np.matrix(gender), occupation_matrix.T)).T.tolist()\n",
    "\n",
    "print('Shape of final user attribute matrix: (list of users with 23 features):', len(user_attributes), len(user_attributes[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98e7SN9qz78v"
   },
   "source": [
    "We then explore the descriptive statistics of the users. These include information related to age (continuous variable), gender (binary variable) and occupation of each user (21, all binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3jJXODCz78w"
   },
   "source": [
    "#### *Descriptive* statistics related to users' <i>age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "colab_type": "code",
    "id": "snuJ7gV_z78w",
    "outputId": "696fbfd9-3b28-4c38-906e-7875a8b40108"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(users['Age'].describe()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcDMUpJZz7-m"
   },
   "source": [
    "#### Percentage of users per <i>gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "dZ1DDpppz7-n",
    "outputId": "23fef21e-01ef-4fb6-928c-c0496cb762b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utl.barplot(['Women', 'Men'], np.array([np.mean(gender) , 1 - np.mean(gender)]) * 100, \n",
    "            'Sex', 'Percentage (%)', \"User's gender\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7WfQi04z7-q"
   },
   "source": [
    "#### Percentage of users per <i>occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "sRcJ4jYbz7-r",
    "outputId": "25ac6cab-d807-4a5d-ee31-758b3497cb3e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(occupation_name, np.mean(occupation_matrix, axis=0) * 100)\n",
    "utl.barplot(attributes, scores, 'Occupation', 'Percentage (%)', \"Users' occupation\", 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMXYOKLOz7-t"
   },
   "source": [
    "### 1.3.2 Movies: Download and preprocesssing\n",
    "\n",
    "In the same way, we process and explore the data associated with the movies. For each movie, we have the title, the release date in North America, as well as the genres with which it is associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "C7LAOhwqz7-u",
    "outputId": "03f847ec-99b6-448d-b652-6a847c8846bc"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv(os.path.join(DATA_DIR, 'u.item'), sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Number of movies\n",
    "nb_movies = len(movies)\n",
    "\n",
    "# Genres\n",
    "movies_genre = np.matrix(movies.loc[:, 5:])\n",
    "movies_genre_name = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u.genre'), sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Quick overview\n",
    "movies.columns = ['Index', 'Title', 'Release', 'The Not a Number column', 'Imdb'] + movies_genre_name.tolist()\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFIZ_Wqmz7_E"
   },
   "source": [
    "We present the ratio of movies according to the genre as a descriptive statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "WoFlTXY4z7_E",
    "outputId": "0e004e94-7b6b-4eb6-910b-7adf5d131674"
   },
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(movies_genre_name, \n",
    "                                   np.array(np.round(np.mean(movies_genre, axis=0) * 1, 2))[0])\n",
    "utl.barplot(attributes, np.array(scores) * 100, xlabel='Genre', ylabel='Percentage (%)', \n",
    "            title=\" \", rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGJIb9Dhz7_Q"
   },
   "source": [
    "### 1.3.3 Ratings: Download and preprocessing\n",
    "\n",
    "The dataset based on users ratings consists of approximately 100k lines (one evaluation per line) where the following are presented: the user identification number, the identification number of the movie, its associated rating and a time marker. The training and test sets were provided as is, that is, we do not need to build them ourselves, and have 80k and 20k evaluations respectively.\n",
    "\n",
    "For practical reasons, we convert the database as a list using the `convert` house function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nx9oYW5gz7_Q",
    "outputId": "d2182a44-5665-43a4-c82c-ad15b09262d7"
   },
   "outputs": [],
   "source": [
    "training_set = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u1.base'), delimiter='\\t'), dtype='int')\n",
    "testing_set = np.array(pd.read_csv(os.path.join(DATA_DIR, 'u1.test'), delimiter='\\t'), dtype='int')\n",
    "\n",
    "print('Example sample (user idx, movie idx, rating, timestamp: ', training_set[0])\n",
    "print('Shape of original training and test set with shape:     ', training_set.shape, testing_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "jy25j9S0QpR1",
    "outputId": "59409f89-3637-41af-a948-35aabfbcfb1b"
   },
   "outputs": [],
   "source": [
    "train_set = utl.convert(training_set, nb_users, nb_movies)\n",
    "test_set = utl.convert(testing_set, nb_users, nb_movies)\n",
    "\n",
    "print('Shape of final training set: (list of users x list of all movies):', len(train_set), len(train_set[0]))\n",
    "print('Shape of final test set:     (list of users x list of all movies):', len(test_set), len(test_set[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mvWDUNez7_W"
   },
   "source": [
    "As we did before, we can get descriptive statistics associated with the evaluations. At first, it might be interesting to study the average trends of users.\n",
    "\n",
    "##### Question 1\n",
    "\n",
    "1. What other types of statistics might be interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "colab_type": "code",
    "id": "72s3zgm_z7_W",
    "outputId": "8e20a0c0-1310-42f2-ac0c-cdf6548fd4b2"
   },
   "outputs": [],
   "source": [
    "train_matrix = np.array(train_set)\n",
    "shape = (len(train_set), len(train_set[0]))\n",
    "train_matrix.reshape(shape)\n",
    "train_matrix_bool = np.where(train_matrix > 0 , 1, 0)\n",
    "\n",
    "user_watch = np.sum(train_matrix_bool, axis=1)\n",
    "pd.DataFrame(user_watch).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opRzwOHIz7_g"
   },
   "source": [
    "Histogram of the number of movies watched per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "y6tRdKbSz7_h",
    "outputId": "216fc169-57cb-478e-f8ac-5bbbc260e94b"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.title('Empirical distribution of \\n the number of movies watched per user')\n",
    "plt.xlabel('Number of movies watched')\n",
    "plt.ylabel('Number of users')\n",
    "plt.hist(user_watch, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpTDzpvBz7_k"
   },
   "source": [
    "Descriptive statistics related to the movies' evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "colab_type": "code",
    "id": "zGmPJgpbz7_l",
    "outputId": "e3c6393d-2d10-4ffa-ffd0-655666401253"
   },
   "outputs": [],
   "source": [
    "movie_frequency = np.mean(train_matrix_bool, axis=0)\n",
    "pd.DataFrame(movie_frequency).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jl3gKAL6z7_o"
   },
   "source": [
    "##### Question 2\n",
    "\n",
    "1. What statistics or observations might we consider relevant? Why?\n",
    "2. What kind of statistics might be more appropriate in such a context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "mRgymfcWz7_p",
    "outputId": "dccc0518-9848-4d92-d59e-e605d5e83cb5"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Proportion of the population who watched the movie')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.hist(movie_frequency, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_WxJwK-z8AD"
   },
   "source": [
    "#### Individual preferences according to the type of movie\n",
    "\n",
    "We could also look at the behavior of a particular individual. Among other things, we could study if there is a bias associated with her evaluation scheme or what are her cinematographic preferences according to the score awarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCy_zSQUz8AF"
   },
   "outputs": [],
   "source": [
    "def stats_user(data, movies_genre, user_id):\n",
    "    \n",
    "    ratings = data[user_id]\n",
    "    stats = np.zeros(6)\n",
    "    eva = np.zeros((6, movies_genre.shape[1]))\n",
    "\n",
    "    for k in np.arange(len(ratings)):\n",
    "        index = int(ratings[k])\n",
    "        stats[index] += 1\n",
    "        eva[index, :] = eva[index, :] + movies_genre[k]\n",
    "\n",
    "    return stats, eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "eZHVMziCz8AJ",
    "outputId": "3e59ee12-2260-416a-af2c-ab7ae1f0506a"
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "stats, eva = stats_user(train_set, movies_genre, user_id)\n",
    "utl.barplot(np.arange(5) + 1, stats[1:6] / sum(stats[1:6]), xlabel='Number of stars', ylabel='Percentage of movies (%)', \n",
    "            title=\" \", rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAI0G1Wnz8A6"
   },
   "source": [
    "##### Question 3\n",
    "\n",
    "1. How can we test to the existence of bias associated to individual's assessment scheme?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4nDaiDoz8A7"
   },
   "source": [
    "## 1.4 Construction of the training and validation sets\n",
    "\n",
    "In machine learning, we manipulate <a href=\"https://blogs.nvidia.com/blog/2018/04/15/nvidia-research-image-translation/\">complex databases</a> for which we attempt to define equally complex function spaces in order to accomplish a specific task. That being said, these function spaces are defined by a set of parameters whose number tends to increase with the complexity of the data. Once space is defined by a set of fixed parameters, we can vary the different values of the hyperparameters in order to empirically explore function spaces. To choose the set of optimal parameters and hyperparameters, we define a metric that allows us to evaluate the model; for example, how much the image of a cat seems likely.\n",
    "\n",
    "To the extent that we want to develop a model that can generalize, the evaluation of its performance must be done on an independent dataset, coming from the same distribution from the set on which the model was trained. This set is called the validation set.\n",
    "\n",
    "<b>! Note! </b>\n",
    "\n",
    "The notion of training and test set in the framework of recommender systems is somewhat different from what is usually seen with so-called supervised problems. If in the context of a supervised problem, the test set consists essentially of new observations (lines from a file) which are independent of observations previously observed in the training set. The paradigm is significantly different when we work with recommendation systems.\n",
    "\n",
    "Indeed, because of the mathematical model on which the recommendation systems are based, the data belonging to the test set are not linked to a new individual, but rather to new evaluations made by the same set of individuals. As a result, the data associated with the training, validation and test sets are no longer independent as assumed (the famous <i>iid</i> hypothesis) which complicates things theoretically.\n",
    "\n",
    "Since the purpose of the workshop is not to study the notion of bias associated with the type of dependence between the different assessments in referral systems, we will naively assume that each of the assessments is independent of each other. Nevertheless, in a practical framework, ignoring this kind of considerations will possibly bias the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9Vgrze6z8A8"
   },
   "outputs": [],
   "source": [
    "def split(data, ratio, tensor=False):\n",
    "    train = np.zeros((len(data), len(data[0]))).tolist()\n",
    "    valid = np.zeros((len(data), len(data[0]))).tolist()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] > 0:\n",
    "                if np.random.binomial(1, ratio, 1):\n",
    "                    train[i][j] = data[i][j]\n",
    "                else:\n",
    "                    valid[i][j] = data[i][j]\n",
    "\n",
    "    return [train, valid]\n",
    "\n",
    "train = split(train_set, 0.8)\n",
    "test = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIG0GWqkz8BC"
   },
   "source": [
    "# 2. Recommender systems: Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkAw6ynxTqmC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sF4nUjipz8Bh"
   },
   "source": [
    "## 2.1 Model\n",
    "\n",
    "Matrix factorization (MF) supposes that each observed evaluation $r_{ui}$ for $1 \\leq u \\leq |U|$ and $1 \\leq i \\leq |I|$, where $|U|$ and $|I|$ are respectively the number of users and movies, can be estimated with respect to a latent (hidden) model. This model presents the estimate $\\hat{r}_{ui}$ of the observed evaluation $r_{ui}$ as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{r}_{ui} =  \\langle p_{u}, q_{i} \\rangle, \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\langle \\cdot \\rangle$ is the scalar product and $p_{u}$ et $q_{i}$ are the latent representations associated to user <i>u</i> and item <i>i</i>. The intuition behind this representation suggests that each evaluation can be estimated by considering a latent characterization of users and items.\n",
    "\n",
    "For example, let's fix the number of latent variables to 3, and suppose that they are associated with the popularity of the movie at the box office, its duration and finally its level of romance. Let us define the <i>u</i> user as a 15-year-old teenager who loves popular and relatively short horror movies. We can model the associated latent vector by:\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{u} = [1, 0, 0]^T.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Suppose now that the movie <i>i</i> turns out to be <i>The Lion King</i> with the following latent modelization:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_{i} = [1, 0.5, 0]^T.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The estimation of the evaluation for this user and item according to the latent representations will therefore be:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{r}_{ui} =  \\langle p_u, q_i \\rangle = 1.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The main challenge in this type of model is to define the set of latent vectors associated with users, grouped in matrix form by $\\mathbf{P}_{|U| \\times k} = [p_1, p_2, .. ., p_k]$, and to items, grouped in matrix form by $ \\mathbf{Q}_{|I| \\times k} = [q_1, q_2, ..., q_k] $.\n",
    " \n",
    "Since the initial problem is to present the most accurate estimates, and thus to calculate $\\mathbf{P}$ and $\\mathbf{Q}$ so as to minimize the distance between the totality of the observed ratings $r_{ui}$ and their estimate $\\hat {r}_{ui}$, we can define the task to accomplish with the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}, \\mathbf{Q} = \\underset{p, q}{\\operatorname{argmin}} \\sum_{r_{ui} \\neq 0} (r_{ui} - \\hat{r}_{ui})^2 = \\underset{p, q}{\\operatorname{argmin}}  \\sum_{r_{ui} \\neq 0} (r_{ui} - \\langle p_u, q_i \\rangle)^2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We may add a regularization on the latent variables, in order to force the associated vectors to have non-zero components:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}, \\mathbf{Q} = \\underset{p, q}{\\operatorname{argmin}} \\sum_{r_{ui} \\neq 0} \\{(r_{ui} - \\langle p_u, q_i \\rangle)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\\},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization hyperparameter, <i>weigth decay</i> in deep learning, or Lagrange multiplier in math. Although this last remark seems technical, it should be noted that latent vectors with very few zero values will in turn lead to predicted ratings other than zero. Since we are trying to propose new recommendations, this constraint seems useful to avoid a sparse matrix estimate $\\hat{\\mathbf{R}}$.\n",
    "\n",
    "In general, the optimization problem above, which turns out to factorize a sparse matrix, cannot be solved as easily as using the least squares in a linear regression context for example. One method will be introduced in this tutorial to estimate $\\mathbf{P}$ and $ \\mathbf{Q}$ matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RkDWAtPz8Bj"
   },
   "source": [
    "## 2.2 Implementation\n",
    "\n",
    "In order to build a recommendation system based on MF, we have to define some specific functions necessary for this type of algorithm. Overall, we can describe the matrix factorization algorithm in three steps:\n",
    "\n",
    "1. <b>Learning loop</b>: Review each of the evaluations in a loop until the estimated model varies only slightly from one iteration to another based on a chosen criterion.\n",
    "\n",
    "2. <b> Estimation</b>: Estimation of the matrices of factors $\\mathbf{P}$ and $\\mathbf{Q}$ respectively associated with the users and the items.\n",
    "\n",
    "3. <b> Evaluation</b>: Evaluation of the performance of the model according to a chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuSCo_qfPRFc"
   },
   "source": [
    "\n",
    "### 2.2.1 Learning loop\n",
    "\n",
    "The function consists simply in iterating the various evaluations until a given stopping criterion is respected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZJrzBL_z8Bn"
   },
   "source": [
    "\n",
    "\n",
    "##### Question 4\n",
    "\n",
    "1. Given estimators obtained via stochastic gradient descent (SGD), what condition should we add to line 31? Why?\n",
    "2. At the end of each epoch, which statistic would it be better to calculate? Code it. Now. Note: Do not forget to initialize objects at the beginning of the function.\n",
    "3. Give two reasons why these statistics are useful.\n",
    "4. The stopping criterion for avoiding overfitting on line 45 is rather stupid. Why?\n",
    "5. Develop a new stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhxDFNIrz8Bo"
   },
   "outputs": [],
   "source": [
    "def learn_to_recommend(data, features=10, lr=0.0002, epochs=101, weigth_decay=0.02, stopping=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       data: every evaluation\n",
    "       features: number of latent variables\n",
    "       lr: learning rate for gradient descent\n",
    "       epochs: number of iterations or maximum loops to perform\n",
    "       weigth_decay: L2 regularization to predict rattings different of 0\n",
    "       stopping: scalar associated with the stopping criterion\n",
    "      \n",
    "     Returns:\n",
    "       P: latent matrix of users\n",
    "       Q: latent matrix of items\n",
    "       loss_train: vector of the different values of the loss function after each iteration on the train\n",
    "       loss_valid: vector of the different values of the loss function after each iteration not on valid\n",
    "       \"\"\"\n",
    "     \n",
    "    train, valid = data[0], data[1]\n",
    "    nb_users, nb_items = len(train), len(train[0])\n",
    "\n",
    "    # Question 4.2: Initialization of lists\n",
    "    # loss_train, loss_valid = [], []\n",
    "\n",
    "    \n",
    "    P = np.random.rand(nb_users, features) * 0.1\n",
    "    Q = np.random.rand(nb_items, features) * 0.1\n",
    "    \n",
    "    for k in range(epochs):        \n",
    "        for i in range(nb_users):\n",
    "            for j in range(nb_items):\n",
    "\n",
    "                # Question 4.1: Code the condition\n",
    "                # if ...\n",
    "                    eij = train[i][j] - prediction(P, Q, i, j)\n",
    "                    P, Q = sgd(eij, P, Q, i, j, features, lr)\n",
    "                               \n",
    "        # Question 4.2: Code the statistics\n",
    "        #\n",
    "        #\n",
    "        \n",
    "        if k % 10 == 0:\n",
    "            print('Epoch : ', \"{:3.0f}\".format(k+1), ' | Train :', \"{:3.3f}\".format(loss_train[-1]), \n",
    "                  ' | Valid :', \"{:3.3f}\".format(loss_valid[-1]))\n",
    "\n",
    "        # Question 4.4: Goofy stopping criterion\n",
    "        if abs(loss_train[-1]) < stopping:\n",
    "            break\n",
    "            \n",
    "        # Question 4.5 : New stopping criterion\n",
    "        # if ... :\n",
    "            break\n",
    "        \n",
    "    return P, Q, loss_train, loss_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgT88hU5z8Bu"
   },
   "source": [
    "### 2.2.2 Loss function\n",
    "\n",
    "The loss function plays a decisive role in the construction of a predictive model. In fact, it is this same cost function that we will try to optimize by iteratively adjusting the values of the latent matrices $\\mathbf{P}$ and $\\mathbf{Q}$.\n",
    "\n",
    "Since we consider that the observed evaluations vary between 1 and 5, the mean squared error (MSE) seems an interesting first option. From a recommender system point of view, we will define the MSE as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "MSE(\\mathbf{R}, \\hat{\\mathbf{R}}) = \\frac{1}{n} \\sum_{r_{ui} \\neq 0} (r_{ui} - \\hat{r}_{ui})^2, \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{R}$ and $\\hat{\\mathbf{R}}$ are respectively the matrices of observed and predicted ratings <i>n</i> is the sum of estimated evaluations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAPyYa9QPDpU"
   },
   "source": [
    "\n",
    "##### Question 5\n",
    "\n",
    "1. Suppose we want to predict the evaluation of user <i>i</i> for the movie <i> j </i>, how should do it? Implement the prediction function.\n",
    "2. An important detail is missing in the following `loss` function. How is this error fundamental? Correct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ft_CJd64z8Bu"
   },
   "outputs": [],
   "source": [
    "# Question 5.1:\n",
    "def prediction(P, Q, i, j):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        P: user matrix\n",
    "        Q: matrix of items\n",
    "        i: index associated with the user i\n",
    "        j: index associated with item j\n",
    "    Returns:\n",
    "        pred: the predicted evaluation of the user i for the item j\n",
    "    \"\"\"\n",
    "    return ?\n",
    "\n",
    "def loss(data, P, Q):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       data: ratings\n",
    "       P: matrix of users\n",
    "       Q: matrix of items   \n",
    "    Returns:\n",
    "        MSE: observed mean of squared errors \n",
    "    \"\"\"\n",
    "    errors_sum, nb_evaluations = 0., 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            \n",
    "            # Question 5.2:\n",
    "            #\n",
    "                errors_sum += pow(data[i][j] - prediction(P, Q, i, j), 2)\n",
    "                nb_evaluations += 1\n",
    "                \n",
    "    return errors_sum / nb_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10bI2dV5z8CQ"
   },
   "source": [
    "### 2.2.3 Estimation\n",
    "\n",
    "Parameters' estimates are directly associated with the loss function we are trying to minimize. With matrix factorization, two estimation techniques are available to calculate latent matrices $\\mathbf{P}$ and $\\mathbf{Q}$ respectively associated to users and items. In both cases, these techniques use the linearity of the matrix factorization model.\n",
    "\n",
    "#### Gradient descent\n",
    "\n",
    "First, we implement stochastic gradient descent (SGD); an iterative method that reviews all non-zero evaluations for each user. Formally, and remembering that the function we are trying to minimize is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{p, q}{\\operatorname{min}} L(\\mathbf{R}, \\lambda) = \\underset{p, q}{\\operatorname{min}} \\sum_{r_{ui} \\neq 0} \\{(r_{ui} - \\langle p_u, q_i \\rangle)^2 + \\lambda \\cdot (||p_u||^2 + ||q_i||^2)\\},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we calculate the gradients of the previous equation as a function of $p_u$ and $q_i$:\n",
    "\n",
    "$$\n",
    "\\nabla_{p_{u}} L(\\mathbf{R}, \\lambda) =  \\epsilon_{ui} \\cdot q_{i} - \\lambda \\cdot p_{u} \\quad \\text{et} \\quad \n",
    "\\nabla_{q_{i}} L(\\mathbf{R}, \\lambda) =  \\epsilon_{ui} \\cdot p_{u} - \\lambda \\cdot q_{i},\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\epsilon_{ui} = r_{ui} - \\hat{r}_{ui}. \n",
    "$$\n",
    "\n",
    "Finally, for each iteration, and insofar as the evaluation observed is non-zero, each update of the latent vectors can be done as follows:\n",
    "\n",
    "$$\n",
    "p_{u}^{(t+1)} \\leftarrow p_{u}^{(t)} + \\gamma \\cdot (\\epsilon_{ui} \\cdot q_{i}^{(t)} - \\lambda \\cdot p_{u}^{(t)}) \\\\\n",
    "q_{i}^{(t+1)} \\leftarrow q_{i}^{(t)} + \\gamma \\cdot (\\epsilon_{ui} \\cdot p_{u}^{(t)} - \\lambda \\cdot q_{i}^{(t)}),\n",
    "$$\n",
    "\n",
    "where $ p_{u}^{(t + 1)}$ is the value of $ p_{u}$ after the $t + 1$ iteration and where $\\gamma$ is the learning rate of the descent.\n",
    "\n",
    "#### Note on alternate least squares\n",
    "\n",
    "The second technique is based on Alternate Least Squares (ALS). This method is elegant in that it allows an analytical form. We will not implement it in this workshop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R7uiumPPLaq"
   },
   "source": [
    "##### Question 6\n",
    "\n",
    "1. Considering the equations associated with the descent of the gradient, complete the `sgd` function to implement the stochastic descent gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ic_KU6uz8Ca"
   },
   "outputs": [],
   "source": [
    "def sgd(error, P, Q, id_user, id_item, features, lr):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        error: difference between observed and predicted evaluation (in that order)\n",
    "        P: matrix of users\n",
    "        Q: matrix of items\n",
    "        id_user: id_user\n",
    "        id_item: id_item\n",
    "        features: number of latent variables\n",
    "        lr: learning for the descent of the gradient\n",
    "       \n",
    "     Returns:\n",
    "         P: the new estimate of P\n",
    "         Q: the new estimate of Q\n",
    "         \"\"\"    \n",
    "    \n",
    "    # Answer 6.1 :\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IwjGI_Bz8Ce"
   },
   "source": [
    "## 2.3 Training\n",
    "\n",
    "Now that matrix factorization is implemented, we can begin to train the model with different parameters and hyperparameters. The idea here is not to adjust the parameters in such a way as to obtain the best model, but simply to understand the role that they can play, both from the point of view of overfitting and computing time. In fact, there are very few wrong answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOMVmv6-z8DQ"
   },
   "outputs": [],
   "source": [
    "features = 5\n",
    "lr = 0.02\n",
    "epochs = 101\n",
    "weigth_decay = 0.02\n",
    "stopping = 0.01\n",
    "\n",
    "P, Q, loss_train, loss_valid = learn_to_recommend(train, features, lr, epochs, weigth_decay, stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAs0VXIoz8DS"
   },
   "source": [
    "Once the model is trained, we can visualize the different learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGJBjTy9z8DT"
   },
   "outputs": [],
   "source": [
    "x = list(range(len(loss_train)))\n",
    "k=0\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.plot(x[-k:], loss_train[-k:], 'r', label=\"Train\")\n",
    "plt.plot(x[-k:], loss_valid[-k:], 'g', label=\"Validation\")\n",
    "plt.title('Learning curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "leg = plt.legend(loc='best', shadow=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZdVEiUmyz8DV"
   },
   "source": [
    "##### Question 7\n",
    "\n",
    "1. Was it necessary to iterate that much?\n",
    "2. Based on these curves, which stopping criterion could we develop?\n",
    "3. How is it more relevant than the one defined in the learning loop?\n",
    "4. Implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7hI0Mzqz8DV"
   },
   "source": [
    "Finally, we can evaluate the performance of our model on the test set.\n",
    "\n",
    "##### Question 8\n",
    "\n",
    "1. Implement the procedure.\n",
    "2. How is it relevant to evaluate performance on such a set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDqJxtywz8DW"
   },
   "outputs": [],
   "source": [
    "# Question 8.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VY2re70Nz8DZ"
   },
   "source": [
    "## 2.4 Analysis\n",
    "\n",
    "**! Note !**\n",
    "\n",
    "Although this section is particularly interesting, you can go directly to Section 3 for a closer look at the algorithmic side of a recommendation. You will be able to return to this section once the matrix factorization workshop has been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9vsE0-Iz8Da"
   },
   "source": [
    "### 2.4.1 Exploring latent layers\n",
    "\n",
    "Thanks to matrix factorization, it is possible to explore the various latent variables associated with users and items. By the nature of the matrices $\\mathbf{P}$ and $\\mathbf{Q}$, the exploration of <i>k</i> latent variables associated with the columns of $\\mathbf{P}$ and $\\mathbf{Q}$ might be interesting.\n",
    "\n",
    "For example, suppose that the first two latent variables in the array of $\\mathbf{Q}$ objects have the following values:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_1 &= [-1.0, \\ -0.8, \\ 0.0, \\ ..., \\ 1.0, \\ 0.5]\n",
    "\\qquad \\text {and} \\qquad\n",
    "q_2 = [-1.0, \\ 0.8, \\ 1.0, \\ ..., \\ 0.5, \\ -0.8].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, suppose that these values correspond to the following movies:\n",
    "\n",
    "1. The Room (2003),\n",
    "2. Star Wars: Attack of the clones (2002),\n",
    "3. Titanic (1997),\n",
    "4. Citizen Kane (1954),\n",
    "5. The Nigthmare before Christmass (1993).\n",
    "\n",
    "By mapping these movies according to the associated values ​​of the first two latent variables, we obtain the following graph:\n",
    "\n",
    "<img src = \"../Images/hidden.png\" width = \"500\">\n",
    "\n",
    "After a quick glance, we can see that the first latent variable is associated with the critical reception of the movie, while the second variable detects the presence of a superstar. If this first analysis may seem interesting to us, but any, it will make more sense once coupled with the values associated with the matrix of users $ \\mathbf{P}$. Suppose now that the first two latent variables of the $\\mathbf{P}$ user matrix have the following values:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_1 &= [1.0, \\ 0.0, \\ -0.5, \\ ..., \\ 1.0, \\ -1.0]\n",
    "\\qquad \\text{and} \\qquad\n",
    "p_2 = [1.0, \\ 0.0, \\ 0.5, \\ ..., \\ -1.0, \\ -0.8].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And that to these values correspond the following users:\n",
    "\n",
    "1. Serena,\n",
    "2. Kali,\n",
    "3. Neil,\n",
    "4. Mary,\n",
    "5. David.\n",
    "\n",
    "We will then be able to map the users according to the values associated with $ p_1 $ and $ p_2 $ while preserving the characterization of the axes that we obtained before:\n",
    "\n",
    "<img src = \"../Images/hidden_2.png\" width = \"500\">\n",
    "\n",
    "This approach could allow us to suggest new movies that have never been evaluated by users simply based on certain characteristics. For example, there is a good chance that Serena loves the upcoming Scorsese <i> The Irish man </i> movie and that Neil looks forward to the new <i> Cat </i>.\n",
    "\n",
    "We therefore propose a function facilitating the exploration of latent variables. Considering the comment related to the viewing frequency of movies in Section 1.4.3, we will only focus on movies with a viewing frequency above a chosen threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDEWyWn_z8Db"
   },
   "outputs": [],
   "source": [
    "def exploration(object_name, matrix, freq, factor, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        object_name: vector of movie names\n",
    "        matrix: latent matrix associated with movies\n",
    "        freq: minimum viewing frequency beyond which we can consider the movie in the analysis\n",
    "        factor: the number of the latent variable that we want to study\n",
    "        k: number of output movies = 3 * k - 1\n",
    "       \n",
    "    Returns:\n",
    "         names: the title of the movies\n",
    "         scores: the associated predicted evaluation\n",
    "         \"\"\"\n",
    "    \n",
    "    values = matrix[:, factor] * freq\n",
    "    names, scores = utl.rearrange(object_name, values)\n",
    "    nonzero = np.nonzero(scores)\n",
    "    \n",
    "    start = nonzero[0][0]    \n",
    "    center = int((len(object_name) - start) / 2)\n",
    "    \n",
    "    names = names[start: (start+k) ] + names[(start+center - 1):(start+ center + 2)] + names[-k:]\n",
    "    scores = scores[start : (start+k) ] + scores[(start+center - 1):(start+ center + 2)] + scores[-k:]\n",
    "    \n",
    "    return names, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZuzfJHfz8Dg"
   },
   "source": [
    "We call the function and visualize the results.\n",
    "\n",
    "##### Question 9\n",
    "\n",
    "1. Can some latent variables be interpretable?\n",
    "2. What would happen if we increase the number of latent variables? If we decrease it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqSJGMbfz8Dg"
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "factor = 0\n",
    "threshold = 0.1\n",
    "names, scores = exploration(movies['Title'], Q, np.where(movie_frequency > threshold, 1, 0), factor, k)\n",
    "\n",
    "df = pd.DataFrame(np.matrix((names, scores)).T, (np.arange(len(scores)) + 1).tolist())\n",
    "df.columns = ['Title', 'Latent factor']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNCN7hBpz8Di"
   },
   "source": [
    "# 3. Applications \n",
    "\n",
    "One of the primary objectives of the recommendation systems is to make personalized recommendations (!) Therefore, it might be interesting to study the recommendations made by the model for a particular user. Naturally, the recommendations made only suggest movies unseen by the user.\n",
    "\n",
    "##### Question 10\n",
    "\n",
    "1. We will now, for a chosen user, make the 10 best recommendations associated with her preferences. Hence, we will proceed in simple steps as presented in the code. Note that the `rearrange` house function presented below may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sFPWtOez8Dj"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example:\n",
    "a, b = ['a', 'b', 'c'], [6,1,3]\n",
    "a, b = rearrange(a, b)\n",
    "print(\"a:\", a, \" b:\", b)\n",
    "\"a:[b, c, a] b:[1,2,6]\"\n",
    "\"\"\"\n",
    "\n",
    "def rearrange(names, ratings):\n",
    "    attribute, scores = [], []\n",
    "    ranking = np.argsort(ratings)\n",
    "\n",
    "    for k in np.arange(len(ranking)):\n",
    "        attribute.append(names[ranking[k]])\n",
    "        scores.append(ratings[ranking[k]])\n",
    "\n",
    "    return attribute, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PI2exdGgz8Dm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "top_what = 10\n",
    "\n",
    "# Step 1: Define which movies have already been viewed in each of the subsets\n",
    "\n",
    "# Step 2: Map the predicted set of ratings for the selected individual\n",
    "\n",
    "# Step 3: Consider only assessments associated with training and validation sets\n",
    "\n",
    "# Step 4: Sort the estimates on the different sets so as to propose the best 10 recommendations\n",
    "\n",
    "# Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16H76jlxz8Dq"
   },
   "source": [
    "It could be interesting to recommend movies to the user according to her preferences but also depending on the genre.\n",
    "\n",
    "##### Question 11\n",
    "\n",
    "1. Write a function to perform such a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pS63d_flz8Dq"
   },
   "outputs": [],
   "source": [
    "def recommendations(user_id, data, P, Q, movies_genre, genre, new):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        user_id: user_id\n",
    "        data: all assessments\n",
    "        P: matrix of individuals\n",
    "        Q: item matrices\n",
    "        movies_genre: kind of movie user wants to listen to\n",
    "        new: Boolean, do we want to make new recommendations or not?\n",
    "    \n",
    "     Returns:\n",
    "        the best suggestions based on the genre of movie selected\n",
    "     \"\"\"\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    return np.array(predictions) * np.array(genre.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVkOjfDhz8Du"
   },
   "outputs": [],
   "source": [
    "genre = \"Animation\"\n",
    "top_what = 5\n",
    "\n",
    "# Calculation and reorganization of recommendations\n",
    "estimate = recommendations(user_id, train, P, Q, movies_genre, genre, False)\n",
    "suggestions, scores = rearrange(np.array(movies['Title']), estimate)\n",
    "\n",
    "# Presentation\n",
    "df = pd.DataFrame(np.matrix((suggestions[-top_what:], scores[-top_what:])).T, (np.arange(top_what) + 1).tolist())\n",
    "df.columns = ['Title', 'Predicted rating']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RSMF - Questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
